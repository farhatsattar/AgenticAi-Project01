# -*- coding: utf-8 -*-
"""project1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iQvTDMVH7d1NsG6h6LJYzfVO4XFMwES4
"""

!pip install google-genai

!pip install --quiet langchain

!pip --quiet install google-generativeai

from google import genai

from google.colab import userdata
GEMINI_API_KEY:str=userdata.get('GOOGLE_API_KEY_4')
if GEMINI_API_KEY:
   print("API key fetched successfully!")
else:
   print("API key not found. Please set the 'GEMINI_API_KEY' user data.")

!pip install --quiet langchain_google_genai

# Initilize and configure the client
# Select the model
from google import genai
from google.genai import Client

client: Client = genai.Client(
    api_key=GEMINI_API_KEY,
)

model: str = "gemini-2.0-flash-exp"

!sudo apt install python3.7

#Define the LLM model Used and some parameters such as temprature and max tokens.Here we will use Gemini 2.0 Flash Model
llm = ChatGoogleGenerativeAI(api_key=GEMINI_API_KEY,
                              model="gemini-2.0-flash-exp",
                              temperature = 0.7 ,
                              tokens= 70

)

prompt_template = PromptTemplate(
    input_variables=["question"],
    template="You are a helpful assistant.Make the response beginer friendly :\n\n{question}"
)

prompt_template2 = PromptTemplate(
    input_variables=["response"],
    template="Write short and concise explanation use bullets where necassery:\n\n{question}"
)

chain = LLMChain(llm=llm, prompt=prompt_template)
chain2 = LLMChain(llm=llm, prompt=prompt_template2)

question = [ "Help me build english speaking chatbot for my app provide me with build intructions"
"tell me tools needed for app"
"suggest me name for my chatbot "]
response = chain.run({"question": question})

print("Answer:", response)

response2 = chain2.run({"question": response})

print("Explanation:", response2)

llm = ChatGoogleGenerativeAI(api_key=GEMINI_API_KEY,
                              model="gemini-2.0-flash-exp",
                              temperature = 0.9 ,
                              tokens= 70

)

prompt_template = PromptTemplate(
    input_variables=["question"],
    template="You are a helpful assistant.Make the response beginer friendly :\n\n{question}"
)

prompt_template2 = PromptTemplate(
    input_variables=["response"],
    template="Write short and concise explanation use bullets where necassery:\n\n{question}"
)

chain = LLMChain(llm=llm, prompt=prompt_template)
chain2 = LLMChain(llm=llm, prompt=prompt_template2)

question = [ "can you tell me about langchain"
]
response = chain.run({"question": question})

print("Answer:", response)

response2 = chain2.run({"question": response})

print("Explanation:", response2)

llm = ChatGoogleGenerativeAI(api_key=GEMINI_API_KEY,
                              model="gemini-2.0-flash-exp",
                              temperature = 0.3 ,
                              tokens= 70

)

prompt_template = PromptTemplate(
    input_variables=["question"],
    template="You are a helpful assistant.Make the response beginer friendly :\n\n{question}"
)

prompt_template2 = PromptTemplate(
    input_variables=["response"],
    template="Write short and concise explanation use bullets where necassery:\n\n{question}"
)

chain = LLMChain(llm=llm, prompt=prompt_template)
chain2 = LLMChain(llm=llm, prompt=prompt_template2)

question = [ "what is agentic AI"
]
response = chain.run({"question": question})

print("Answer:", response)

response2 = chain2.run({"question": response})

print("Explanation:", response2)